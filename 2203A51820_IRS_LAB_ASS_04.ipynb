{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aORBa3DTK7l8"
      },
      "source": [
        "Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAMpFAzxLATs",
        "outputId": "36d1de13-661c-4e84-db81-ef23530395ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text without Punctuation: Artificial Intelligence is shaping the future of technology\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "# Example text\n",
        "text = \"Artificial Intelligence is shaping the future of technology.\"\n",
        "\n",
        "# Removing punctuations\n",
        "no_punct_text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "# Output results\n",
        "print(\"Text without Punctuation:\", no_punct_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93M4ZAbnLEwn"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehIZVvj0LWhH",
        "outputId": "f33f74cb-1b5b-4490-ddea-7341c3451394"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Tokens: ['Artificial', 'Intelligence', 'is', 'shaping', 'the', 'future', 'of', 'technology', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "\n",
        "# Download necessary data\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def tokenize_words(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "def tokenize_characters(text):\n",
        "    return list(text)\n",
        "\n",
        "# Example text\n",
        "text = \"Artificial Intelligence is shaping the future of technology.\"\n",
        "\n",
        "# Tokenization\n",
        "word_tokens = tokenize_words(text)\n",
        "\n",
        "# Output results\n",
        "print(\"Word Tokens:\", word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYooPK_0LbSD"
      },
      "source": [
        "Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrYOPqJWLd1N",
        "outputId": "b5530ab5-c426-438c-e7ee-89bdceacf36b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Tokens (without stop words): ['Artificial', 'Intelligence', 'shaping', 'future', 'technology', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Removing Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Output results\n",
        "print(\"Filtered Tokens (without stop words):\", filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvJphRMnLjGZ"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glh8y1qFLj0-",
        "outputId": "8f271374-e478-40f3-a5eb-5e6456fc86f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed Tokens: ['artifici', 'intellig', 'shape', 'futur', 'technolog', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# Output results\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joORwoOeLmpz"
      },
      "source": [
        " Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGBBBctlLrpD",
        "outputId": "9645290d-52b0-4c03-eb81-9b8ac4ea098f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatized Tokens: ['Artificial', 'Intelligence', 'shaping', 'future', 'technology', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "# Output results\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziBnABTeLwIU"
      },
      "source": [
        "BI-GRAM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_BmnauBLy36",
        "outputId": "78efa7f1-7ad8-4f0d-e00a-e683181b97a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigrams: [('Artificial', 'Intelligence'), ('Intelligence', 'shaping'), ('shaping', 'future'), ('future', 'technology'), ('technology', '.')]\n"
          ]
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "# Generating BI-GRAMs\n",
        "bigrams = list(ngrams(filtered_tokens, 2))\n",
        "\n",
        "# Output results\n",
        "print(\"Bigrams:\", bigrams)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHdU06ESL2ja"
      },
      "source": [
        "TRI-GRAM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Br0enHL5no",
        "outputId": "72cf0dca-ff8a-45f6-d5a0-87fa7bd4566f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trigrams: [('Artificial', 'Intelligence', 'shaping'), ('Intelligence', 'shaping', 'future'), ('shaping', 'future', 'technology'), ('future', 'technology', '.')]\n"
          ]
        }
      ],
      "source": [
        "# Generating TRI-GRAMs\n",
        "trigrams = list(ngrams(filtered_tokens, 3))\n",
        "\n",
        "# Output results\n",
        "print(\"Trigrams:\", trigrams)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
